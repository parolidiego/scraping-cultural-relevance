---
title: "What was going on the day you were born?"
author: "Diego Paroli, David Pereiro Pol"
output: 
  prettydoc::html_pretty:
    theme: architect
    toc: yes
---

# Setting up the environment

## Installing packages

The chuck below automatically installs the required packages in case they are not installed already.

```{r}
# Cleaning environment
rm(list = ls())

# Set CRAN mirror
options(repos = c(CRAN = "https://cran.rstudio.com"))

# List of packages needed
packages <- c("xml2",
              "httr",
              "httr2",
              "tidyverse",
              "rvest",
              "janitor",
              "glue",
              "usethis",
              "rstudioapi")

# Checks if the packages are already installed, otherwise installs them
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}
```

## Storing API keys

1.  Running the code below will open a `.Renviron` file in which you can store safely your API keys

```{r, eval = FALSE}
# Opening .Renviron file to store the API keys
usethis::edit_r_environ()
```
2.   With the `.Renviron` file now open it is time to store in it your API keys. Insert the keys within the quotes, copy the 2 lines below and paste them in the `.Renviron` file.

API_KEY_NYT = "your_key_here"

API_KEY_GUARDIAN = "your_key_here"

3.   Once the `.Renviron` contains the keys it is time to save it. Press CTRL+S (Windows) or Command+S (MacOS) and then close it.

4.   Restart the R session with the code below for the changes to come into effect.

```{r, eval=FALSE}
rstudioapi::restartSession(clean = TRUE)
```

## Loading libraries

```{r, warning=FALSE, message=FALSE}
packages <- c("xml2",
              "httr",
              "httr2",
              "tidyverse",
              "rvest",
              "janitor",
              "glue",
              "usethis",
              "rstudioapi")
# Loading the packages
invisible(lapply(packages, library, character.only = TRUE))
```

## Adjusting other settings

```{r, message = FALSE}
# Setting english as the system language
invisible(Sys.setlocale("LC_TIME", "English_United States.UTF-8"))
```

## Setting your user agent

You can easily find out your user agent by searching "my user agent" on Google. Once you get this information paste it before the semicolon below. Also write within the string your name, last name and email. 

Although this passage is not required, it allows websites to contact you if you are causing problems to them with your requests and therfore it is best practice to do it.

```{r, eval = FALSE}
my_user_agent <- "user agent; Name Last-Name / email"
set_config(user_agent(my_user_agent))
# It should look like this:
fake_user_agent <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.41; Pablo Gonzalez / pablo.gonzalez@gmail.com"
```

# Billboard Hot 100 scraper

Scraping the Billboard Hot 100 chart has been fairly straightforward. The chart is updated weekly, the website includes the publication date in its URL and the structure of the website remains the same across the different years of the chart.

Therefore we were able to create a vector of dates to be pasted into the URL, loop over it and access the information we wanted for each edition of the chart.

Since we wanted to focus on building an app showcasing multiple information about dates in the past, we focused on scraping the most amount of information possible for the song that was no.1 in each edition of the chart.

## Setting start and end date

Enter your desired start date and end date below in the format "yyyy-mm-dd", within quotation marks.

To ensure the accuracy of the information, the start date should be a Saturday.

The first Hot 100 chart was published on August 4, 1958. However, we do not guarantee reproducibility for years before the 2000s as the publication date of the chart and/or the website structure may change for years earlier than 2000.

```{r}
start_date <- "2000-01-01" # Make sure this is a saturday
end_date <- "2000-03-31" # This date can be any date
```

## The scraper

```{r}
# Range of dates
dates <- seq(as.Date(start_date), as.Date(end_date), by="week")

# Initialize an empty list to store results
billboard_data <- list()

# Loop over dates
for (date in dates) {
  
  # System sleep
  Sys.sleep(2)
  
  # Format date for URL (YYYY-MM-DD) and paste it into the link
  date_str <- as.character(format(as.Date(date), "%Y-%m-%d"))
  link <- paste0("https://www.billboard.com/charts/hot-100/", date_str, "/")
  
  # Read html
  link_read <- read_html(link)
  
  # Artist and song
  artistsong <- xml_find_all(link_read, "//li[@class='o-chart-results-list__item // lrv-u-flex-grow-1 lrv-u-flex lrv-u-flex-direction-column lrv-u-justify-content-center lrv-u-border-b-1 u-border-b-0@mobile-max lrv-u-border-color-grey-light  lrv-u-padding-l-1@mobile-max']")
  
  song <- xml_text(xml_children(artistsong)[1])
  artist <- xml_text(xml_children(artistsong)[2])
  
  
  # Image
  image_urls <- xml_find_all(link_read, "//div[@class='lrv-a-crop-1x1 a-crop-67x100@mobile-max']/img[@class='c-lazy-image__img lrv-u-background-color-grey-lightest lrv-u-width-100p lrv-u-display-block lrv-u-height-auto']")
  
  image_url <- xml_attr(image_urls[[1]], "src")
  
  
  # Last weeks and weeks on the chart info
  weeks_info <- xml_find_all(link_read, "//li[@class='o-chart-results-list__item // a-chart-color u-width-72 u-width-55@mobile-max u-width-55@tablet-only lrv-u-flex lrv-u-flex-shrink-0 lrv-u-align-items-center lrv-u-justify-content-center lrv-u-border-b-1 u-border-b-0@mobile-max lrv-u-border-color-grey-light u-background-color-white-064@mobile-max u-hidden@mobile-max']/span")
  
  last_week <- xml_text(weeks_info[[1]])
  weeks_on_chart <- xml_text(weeks_info[[2]])
  
  
  # Number of weeks at no.1
  no1_weeks <- xml_find_all(link_read, "//div/span[@class='c-label  a-font-primary-bold-xxl lrv-u-padding-lr-1 lrv-u-padding-t-025']")
  weeks_at_1 <- ifelse(length(no1_weeks) > 0, xml_text(no1_weeks[[2]]), NA)
  
  
  # Group data in a tibble
  data <- tibble(
    date = date_str,
    song = song,
    artist = artist,
    image_url = image_url,
    last_week = last_week,
    weeks_on_chart = weeks_on_chart,
    weeks_at_1 = weeks_at_1
  )
  
  # Append to results list
  billboard_data[[date_str]] <- data
}

# Combine all results into one dataframe
billboard_data <- bind_rows(billboard_data)

# Clean columns
billboard_clean <- billboard_data  %>% 
  mutate(across(c(song, artist, last_week, weeks_on_chart, weeks_at_1),
                ~ str_squish(.x)))

dim(billboard_clean)
head(billboard_clean)
```

We have run the scraper from 2000-01-01 to 2024-12-31 and stored our result in the following file.

```{r, eval = FALSE}
write.csv(billboard_clean, "data/billboard_database.csv")
```

# Box Office Mojo scraper

Box Office Mojo is a website storing information about the best selling movies at the box office. 

Again with the purpose of displaying information for each day we decided to scrape a table containing the best selling movie of that day in the USA for each day of a year. In this case as well, we were able to loop over multiple years since the URL included an indicator specifying the year that was displayed by the website.

However, this table contained little additional information about the movie that was no.1, therefore we decided to access the page of each of the no.1 movies to get more information. We built a function to retrieve the links of the personal page of each movie from the initial table and then, using those links, we scraped all the information that we deemed interesting from the personal page of the movies.

## Setting start and end date

Insert the year below with format yyyy without quotation marks.

Notice that the Box Office Mojo scraper operates on a yearly basis, so it will retrieve data for full years at a time.

Box Office Mojo provides data starting from 1980, but we have not verified or tested our scraper with data prior to the 2000s.

```{r}
start_date <- 2000
end_date <- 2001
```

## The scraper

Getting the best selling movie for each day.

```{r}
# Convert the dates before to years
years <- seq(end_date, start_date, by = -1)

# Build the urls from the years
urls <- glue("https://www.boxofficemojo.com/daily/{years}/?view=year")

# Get the best selling movie by day
movies <- function(url, year) {
  Sys.sleep(2)
  
  page <- read_html(url)
  
  # Extract the table and select relevant columns
  movies <- page |> 
    html_table() %>%
    .[[1]] |> 
    select(`#1 Release`, Gross, Date) 
  
  # Clean the columns
  movies <- movies  |> 
    mutate(day = gsub("\\D", "", substr(Date, 1, 6)),
           month = substr(Date, 1, 3),
           year = year) |>  
    mutate(date = paste(day, month, year, sep = " "),
           date = as.Date(date, format = "%d %b %Y")) |> 
    select(-c(Date, day, month, year)) |> 
    mutate(Gross = as.numeric(gsub("\\D", "", Gross))) |> 
    rename(
      daily_earnings = Gross,
      best_selling = "#1 Release"
    )
  
  return(movies)
}

movies_per_day <- map2_dfr(urls, years, movies)
dim(movies_per_day)
head(movies_per_day)
```

Retrieving the link to the personal page of each movie that was the top daily seller.

```{r}
# Get the links of all top selling movies to access their info
links <- function(url) {
  Sys.sleep(2)
  
  page <- read_html(url)
  
  links <- page |> 
    xml_find_all("//td[@class = 'a-text-left mojo-field-type-release mojo-cell-wide']//a") |> 
    xml_attr(attr = "href") |> 
    str_remove_all("_\\d+$") |> 
    unique() %>% 
    paste0("https://www.boxofficemojo.com", .)
  
  return(links)
}

links_for_info <- map(urls, links)
# Unlisting the results to pass them to the next function
links <- links_for_info |> unlist()
```

Extracting additional information about each top movie.

```{r}
# Extract additional info about each top selling movie personal page
extract_movie_info <- function(url) {
  Sys.sleep(2)
  
  ind_movie <- read_html(url)
  
  # Scraping the title
  title <- ind_movie |> 
    xml_find_all("//h1[@class = 'a-size-extra-large']") |> 
    xml_text() 
  
  # Scraping the movie's description
  description <- ind_movie |> 
    xml_find_all("//p[@class='a-size-medium']")
  description <- ifelse(length(description) >1, NA, xml_text(description))
  
  # Scraping the url of the image
  image <- ind_movie |> 
    xml_find_all("//div[@class = 'a-fixed-left-grid']//img") |> 
    xml_attr(attr = "data-a-hires") %>%
    .[[1]]
  
  # Scraping the earning box on the left (Grosses)
  money_elements <- ind_movie |> xml_find_all("//span[@class='money']")
  worldwide_earnings <- ifelse(length(money_elements) >= 3, 
                               xml_text(money_elements[[3]]), NA)
  opening_day_earnings <- ifelse(length(money_elements) >= 4, 
                                 xml_text(money_elements[[4]]), NA)
  
  # Extracting all info from the rest of the box as a vector
  # Middle column are odd numbers and right column is even numbers
  info <- ind_movie |>
    xml_find_all("//div[@class = 'a-section a-spacing-none mojo-summary-values mojo-hidden-from-mobile']//span[not(@*)]") |>
    xml_text(trim = TRUE)
  
  if (length(info) == 0) {
    
    return(tibble(title, description, image, 
                  worldwide_earnings, opening_day_earnings))
    
  }
  
  # Grabbing all possible information (not always every page has all the info)
  cols <- info[seq(1, length(info), by = 2)]
  values <- info[seq(2, length(info), by = 2)]
  
  info_tibble <- as_tibble(as.data.frame(t(values)))
  colnames(info_tibble) <- cols
  
  # Adding the data scraped in the first part
  info_tibble <- info_tibble |> 
    mutate(opening_day_earnings = opening_day_earnings, .before = everything()) |> 
    mutate(worldwide_earnings = worldwide_earnings, .before = everything()) |> 
    mutate(image = image, .before = everything()) |>
    mutate(description = description, .before = everything()) |> 
    mutate(title = title, .before = everything())
  
  return(info_tibble)
}

best_selling_info <- map(links, extract_movie_info)
best_selling_info <- bind_rows(best_selling_info)

# Cleaning the data
best_selling_info_cleaned <- best_selling_info |> 
  mutate(
    worldwide_earnings = as.numeric(str_replace_all(worldwide_earnings, "\\D", "")),
    opening_day_earnings = as.numeric(str_replace_all(opening_day_earnings, "\\D", "")),
    Budget = as.numeric(str_replace_all(Budget, "\\D", "")),
    Distributor = str_replace_all(Distributor, "See full.*", ""),
    Genres = str_replace_all(Genres, "\\n|\\s{2,}", "-"),
    `Release Date` = str_replace_all(`Release Date`, "\\n|\\s{2,}", " ")) |>
  mutate(Genres = str_replace_all(Genres, "--", " - ")) |> 
  select(-c("Release Date\n        \n            (Wide)", "IMDbPro", "Opening")) |> 
  rename(
    distributor = Distributor,
    release_date = `Release Date`,
    mpaa_rating = MPAA,
    running_time = `Running Time`,
    genres = Genres,
    in_release = `In Release`,
    widest_release = `Widest Release`,
    budget = Budget
  )
head(best_selling_info_cleaned)
```

Joining the daily data with the specific information about each daily best selling movie.

```{r, warning=FALSE}
# Joining the additional information with the daily best seller
final_best_selling_movies <- movies_per_day |> 
  left_join(best_selling_info_cleaned, by = join_by("best_selling"=="title"))
head(final_best_selling_movies)
```

We have run the scraper from 2000 to 2023 (included) and stored our result in the following file.

```{r, eval = FALSE}
write.csv(final_best_selling_movies, "data/box_office_database.csv")
```

# The New York Times API

We want to retrieve front-page news articles from the New York Times API, limiting ourselves for now to just 1 article (hopefully the most important) per day. The API returns 10 articles per call and we are able to rank them by relevance.

We could make a single query for each day, but we have limits on the number of query per day. Therefore to collect a larger amount of data in a shorter time period we decided to query over a two-day period. The 2 days period allows us to extract the information we wanted (the top article of each day) for more than one day per query, without loosing too many datapoints, which can happen if the top article from a specific day falls outside the top 10 most relevant results for the entire two-day period. This decision is primarily driven by time constraints and the submission deadline, but we aim and plan to build a full dataset in the future.

## Setting start and end date

The function below is necessary to implement the approach discussed above. It generates two vectors: one vector stores all the values that are going to be passed on as start dates parameters of our queries, the other vector stores all the values that are going to be passed on as end dates parameters of our queries. It requires to arguments, the first will be the start date of the first query and the second one will represent the upper boundary for the sequence of start dates.

```{r}
# Create a function to get start and end date for every query
dates_creator <- function(start, end) {
  # To be provided as "yyyy-mm-dd"
  
  # Sequence of start dates
  start_dates <- seq(as.Date(start), as.Date(end), by="2 days")
  # Sequence of end dates
  end_dates <- start_dates + 1
  
  # Modifies the dates to get them in the format wanted by the NYT API
  start_dates <- start_dates |> 
    as.character() %>%
    str_remove_all(., "-") |> 
    as.numeric()
  
  end_dates <- end_dates |> 
    as.character() %>%
    str_remove_all(., "-") |> 
    as.numeric()
  
  list_dates <- list(start_dates, end_dates)
  
  return(list_dates)
}
```

The New York Times API has a limit of queries set at 500 per day and maximum 5 per minutes.

Each of our query spans 2 days therefore our first start and our last start date (the two arguments that we need to provide to the function) date must be at maximum 999 days apart.

The arguments need to be provided as "yyyy-mm-dd".

```{r}
dates <- dates_creator("2000-01-01", "2000-01-21")

start_dates <- dates[[1]]
start_dates
end_dates <- dates[[2]]
end_dates
```

## Talking to the API

```{r}
ny_news <- function(start_date, end_date){
  api_key <- Sys.getenv("API_KEY_NYT")
  if (api_key == "") stop("API key is missing! Set 'API_KEY_NYT' in your environment.")
  
  Sys.sleep(12)
 
  main_link <- "https://api.nytimes.com/svc/search/v2/articlesearch.json"
  
  # Building and performing the API query 
  data <- main_link |> 
  request() |>
  req_url_query(`api-key` = Sys.getenv("API_KEY_NYT"), 
                begin_date = start_date,
                end_date = end_date, 
                fq = 'print_page:1 AND print_section:("A", "1") AND type_of_material:"News"',
                sort = 'relevance') |>
  req_perform() |> 
  resp_body_json(simplifyVector = TRUE)
  
  # Selecting relevant data
  news <- tibble(date = data$response$docs$pub_date,
                 headline = data$response$docs$headline$main,
                 print_headline = data$response$docs$headline$print_headline,
                 abstract = data$response$docs$abstract,
                 snippet = data$response$docs$snippet,
                 lead_paragraph = data$response$docs$lead_paragraph)
  
  # Selecting only the top ranked news per day
  news <- news |> 
    mutate(date = str_remove_all(date,"T.+$")) |> 
    mutate(date = str_trim(date)) |>     
    mutate(date = as_date(date)) |>  
    mutate(rank = row_number())  |> 
    group_by(date) |> 
    slice_min(rank) |> 
    ungroup() |> 
    select(-rank)
  
  return(news)
}

news_nyt <- map2_dfr(start_dates, end_dates, ny_news)
head(news_nyt)
```

We have run this function to make repeated calls (always within the limits) to the API for several days. Getting data from 2000-01-01 to 2005-12-31 so far. We plan to expand this in the future.

```{r, eval=FALSE}
if (!file.exists("data/nyt_database.csv")) {
    write_csv(news_nyt, "data/nyt_database.csv")
  } else {
    # If the file does exist, then append the result to the already existing CSV file
    write_csv(news_nyt, "data/nyt_database.csv", append = TRUE)
  }
```

# The Guardian API

For The Guardian we applied a similar reasoning as for The New York Times. However this time, each API calls gets  us back up to 50 results. Therefore we expanded our query window to 5 days instead of 2 days.

## Setting start and end date

The function is structured as before with only minor changes to format dates as needed for the API call.

```{r}
# Create a function to get a list of start and end date for every query
dates_creator <- function(start, end) {
  # To be provided as "yyyy-mm-dd"
  
  start_dates <- seq(as.Date(start), as.Date(end), by="5 days")
  end_dates <- start_dates + 4
  
  start_dates <- start_dates |> 
    as.character() 
  
  end_dates <- end_dates |> 
    as.character() 
  
  list_dates <- list(start_dates, end_dates)
  
  return(list_dates)
}
```

The Guardian API has a limit of queries set at 500 per day and maximum 60 per minutes.

Each of our query spans 5 days therefore our first start and our last start date (the two arguments that we need to provide to the function) date must be at maximum 24999 days apart.

Again the arguments need to be provided as "yyyy-mm-dd".

```{r}
# Apply function
dates <- dates_creator("2000-01-01", "2000-01-21")

start_dates <- dates[[1]]
start_dates
end_dates <- dates[[2]]
end_dates
```

## Talking to the API

```{r}
guardian_news <- function(start_date, end_date){
  
  api_key <- Sys.getenv("API_KEY_GUARDIAN")
  if (api_key == "") stop("API key is missing! Set 'API_KEY_GUARDIAN' in your environment.")
   
  Sys.sleep(1)
  
  main_link <- "https://content.guardianapis.com/search"
  
  # Building and performing the API query 
  data <- main_link |> 
    request() |>
    req_url_query(`api-key` = Sys.getenv("API_KEY_GUARDIAN"), format = "json",
                `from-date` = start_date, `to-date` = end_date, 
                `order-by` = "relevance", `section` = "world", 
                `show-fields` = "headline,trailText,standfirst",
                `page-size` = 50) |>
    req_perform() |> 
    resp_body_json(simplifyVector = TRUE)
  
  # Selecting relevant data
  news <- tibble(
    date = data$response$results$webPublicationDate,
    title = data$response$results$webTitle,
    subtitle = data$response$results$fields$trailText,
    standfirst = data$response$results$fields$standfirst,
    url = data$response$results$webUrl
  )
  
  # Selecting only the top ranked news per day
  news <- news |> 
    mutate(date = str_remove_all(date,"T.+$"))|> 
    mutate(date = str_trim(date)) |>     
    mutate(date = as_date(date)) |>  
    mutate(rank = row_number())  |> 
    group_by(date) |> 
    slice_min(rank, with_ties = FALSE) |> 
    ungroup() 
  
  return(news)
}

news_guardian <- map2_dfr(start_dates, end_dates, guardian_news)
head(news_guardian)
```

We have run this function to make repeated calls (always within the limits) to the API for several days. Getting data from 2000-01-01 to 2007-12-31. We plan to extend this datset as well in the future.

```{r, eval=FALSE}
if (!file.exists("data/guardian_database.csv")) {
    write_csv(news_guardian, "data/guardian_database.csv")
  } else {
    # If the file does exist, then append the result to the already existing CSV file
    write_csv(news_guardian, "data/guardian_database.csv", append = TRUE)
  }
```

# We got the data. What now?

With the data that we obtain we have built a Shiny app where one can choose a date and the data for that date will be displayed.

You can see our Shiny app by executing the `shiny_app.R` script in this directory. By running the code below you will automatically open the script which then needs to be executed fully to launch the Shiny App.

Enjoy!

```{r, eval=FALSE}
file.edit("shiny_app.R")
```

